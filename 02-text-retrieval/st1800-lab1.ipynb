{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# cargar las librerias necesarias\n","## 1. nltk para 'procesamiento natural del lenguaje'\n","## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n","## 3. re - expresiones regulares\n","## 4. numpy, codecs, etc - otraas"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import nltk\n","import pandas as pd\n","import numpy as np\n","import re\n","import codecs\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# directorios (path) de entrada y salida:\n","# \n","path_in=\"../datasets/papers_sample/\"\n","#\n","# crear este directorio /tmp/papers_out\n","path_out=\"/tmp/papers_out/\""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# corpus de nltk para 'tokenizer' y 'stopwords'\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"# ejemplo de como nltk tokeniza:\ntexto=\"texto libre weren't que permite--4 crear  las   hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\ntokens = nltk.word_tokenize(texto)\nprint(len(tokens))\nprint(tokens)"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"# note la estrategia de tokenizar con sentencias simples de python, \n# ¿ cual le parece mejor?\n# y note la diferencia entre .split() y .split(' ')\ntexto=\"texto libre weren't que permite--4 crear  las   hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\ntokens = texto.split()\nprint(len(tokens))\nprint(tokens)\ntokens = texto.split(' ')\nprint(len(tokens))\nprint(tokens)"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# otra libreria diferentes de nltk para diccionario de stopwords, cual será mejor?\n","# $ git clone --recursive git://github.com/Alir3z4/python-stop-words.git\n","#!pip install stop-words --user\n","\n","from stop_words import get_stop_words\n","stop_words = get_stop_words('english')\n","stop_words = get_stop_words('en')\n","print(len(stop_words))\n","print(stop_words)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# stopwords en nltk\n","from nltk.corpus import stopwords\n"," \n","stop_words_nltk = set(stopwords.words('english'))\n","print(len(stop_words_nltk))\n","print(stop_words_nltk)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n","from nltk.corpus import words as voc_en\n","x = len(voc_en.words())\n","print('tamaño del diccionario en ingles del nltk: ',x)\n","# verifica si una palabra pertenece al diccionario:\n","w = \"house\"\n","if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n","    print(w,\" true\")\n","else:\n","    print(w,\" false\")\n","    \n","w = \"pepito\"\n","if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n","    print(w,\" true\")\n","else:\n","    print(w,\" false\")    "]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":"# leer un archivo de ejemplo en .txt\ninput_file = open(path_in+\"0704.3504.txt\", \"r\", encoding='iso-8859-1')\n#output_file_filtered = open(path_out+\"0704.3504_filtered.txt\", \"w\")\n#output_file_drop = open(path_out+\"0704.3504_drop.txt\", \"w\")\nfiledata = input_file.read()"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["# opción 1:\n","# TOKENIZAR con .split(), \n","# ELIMINAR tokens de long = 1\n","# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n","# REMOVER stop words de nltk\n","# graficar los 20 términos más frecuentes:\n","\n","tokens = filedata.split()\n","print(len(tokens))\n","tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n","# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n","tokens = [w.lower() for w in tokens if len(w)>1]\n","tokens = [w for w in tokens if w not in stop_words_nltk]\n","\n","fdist = nltk.FreqDist(tokens)\n","print('numero de palabras finales = ',len(fdist))\n","topwords = fdist.most_common(20)\n","print (topwords)\n","x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# opción 2:\n","# TOKENIZAR con nltk, \n","# ELIMINAR tokens de long = 1\n","# ELIMINAR caracteres que no sean alfanumericos\n","# REMOVER stop words\n","# graficar los 20 términos más frecuentes:\n","\n","tokens = nltk.word_tokenize(filedata)\n","tokens = [w.lower() for w in tokens if len(w)>1]\n","tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n","tokens = [w for w in tokens if w not in stop_words_nltk]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('numero de palabras finales = ',len(fdist))\n","print (topwords)\n","x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# Stemming con NLTK\n","\n","from nltk.stem import PorterStemmer\n","from nltk.stem import LancasterStemmer\n","\n","porter = PorterStemmer()\n","lancaster = LancasterStemmer()\n","\n","#tokens = [porter.stem(w) for w in tokens]\n","tokens = [lancaster.stem(w) for w in tokens]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('numero de palabras finales = ',len(fdist))\n","x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Lemmatization con NLTK\n","\n","from nltk.stem import WordNetLemmatizer\n","\n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n","#tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n","\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('numero de palabras finales = ',len(fdist))\n","x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"# leer un archivo de ejemplo en .txt\ninput_file = open(path_in+\"0704.3504.txt\", \"r\", encoding='iso-8859-1')\noutput_file_clean = open(path_out+\"0704.3504_clean.txt\", \"w\")"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for line in input_file:\n","    line_clean = \"\"\n","    \n","    tokens = line.split()\n","    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n","    tokens = [w.lower() for w in tokens if len(w)>1]\n","    tokens = [w for w in tokens if w.isalpha()]\n","    tokens = [w for w in tokens if w not in stop_words_nltk]\n","    \n","    for w in tokens:\n","        line_clean=line_clean+w+\" \"\n","            \n","    if (line_clean!=\"\"):\n","        line_clean=line_clean+\"\\n\"\n","        output_file_clean.write(line_clean)\n","output_file_clean.close()        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"input_file_clean = open(path_out+\"0704.3504_clean.txt\", \"r\", encoding='iso-8859-1')"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filedata = input_file_clean.read()\n","tokens = filedata.split()\n","fdist = nltk.FreqDist(tokens)\n","topwords = fdist.most_common(20)\n","print('numero de palabras finales = ',len(fdist))\n","x,y = zip(*topwords)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["word_freq = fdist.most_common(len(fdist))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import csv\n\nwith open(path_out+'0704.3504_tf.csv', 'w') as csvFile:\n    writer = csv.writer(csvFile)\n    writer.writerow([\"word\", \"frecuency\"])\n    writer.writerows(word_freq)\n\ncsvFile.close()"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# extract top 30 words\n","top_words = word_freq[:20]\n","print(top_words)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","df = pd.DataFrame(top_words)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","x,y = zip(*top_words)\n","plt.figure(figsize=(15,10))\n","plt.bar(x,y)\n","plt.xticks(rotation=90)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","df = pd.DataFrame(top_words)\n","plt.figure(figsize=(15,10))\n","plt.bar(df[0],df[1])\n","plt.xticks(rotation=45)\n","plt.xlabel(\"Word\")\n","plt.ylabel(\"frecuency\")\n","plt.show()"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}