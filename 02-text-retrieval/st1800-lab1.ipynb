{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar las librerias necesarias\n",
    "## 1. nltk para 'procesamiento natural del lenguaje'\n",
    "## 2. pandas para procesamiento de dataframes, muy usado en preparación de datos\n",
    "## 3. re - expresiones regulares\n",
    "## 4. numpy, codecs, etc - otraas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorios (path) de entrada y salida:\n",
    "# \n",
    "path_in=\"../datasets/papers_sample/\"\n",
    "#\n",
    "# crear este directorio /tmp/papers_out\n",
    "path_out=\"/tmp/papers_out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus de nltk para 'tokenizer' y 'stopwords'\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo de como nltk tokeniza:\n",
    "texto=\"texto libre que permite crear     hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note la estrategia de tokenizar con sentencias simples de python, \n",
    "# ¿ cual le parece mejor?\n",
    "# y note la diferencia entre .split() y .split(' ')\n",
    "texto=\"texto libre que permite crear     hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\n",
    "tokens = texto.split()\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "tokens = texto.split(' ')\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otra libreria diferentes de nltk para diccionario de stopwords, cual será mejor?\n",
    "# $ pip install stop-words\n",
    "# $ git clone --recursive git://github.com/Alir3z4/python-stop-words.git\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "stop_words = get_stop_words('en')\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stop-words --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords en nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(len(stop_words_nltk))\n",
    "print(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n",
    "from nltk.corpus import words as voc_en\n",
    "x = len(voc_en.words())\n",
    "print('tamaño del diccionario en ingles del nltk: ',x)\n",
    "# verifica si una palabra pertenece al diccionario:\n",
    "w = \"house\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")\n",
    "    \n",
    "w = \"pepito\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer un archivo de ejemplo en .txt\n",
    "input_file = open(path_in+\"0704.3504.txt\", \"r\", encoding='iso-8859-1')\n",
    "#output_file_filtered = open(path_out+\"0704.3504_filtered.txt\", \"w\")\n",
    "#output_file_drop = open(path_out+\"0704.3504_drop.txt\", \"w\")\n",
    "filedata = input_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 1:\n",
    "# TOKENIZAR con .split(), \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n",
    "# REMOVER stop words de nltk\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = filedata.split()\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "topwords = fdist.most_common(20)\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 2:\n",
    "# TOKENIZAR con nltk, \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos\n",
    "# REMOVER stop words\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = nltk.word_tokenize(filedata)\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming con NLTK\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "#tokens = [porter.stem(w) for w in tokens]\n",
    "tokens = [lancaster.stem(w) for w in tokens]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization con NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n",
    "tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer un archivo de ejemplo en .txt\n",
    "input_file = open(path_in+\"0803.2570.txt\", \"r\", encoding='iso-8859-1')\n",
    "output_file_clean = open(path_out+\"0803.2570_clean.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in input_file:\n",
    "    line_clean = \"\"\n",
    "    \n",
    "    tokens = line.split()\n",
    "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "    \n",
    "    for w in tokens:\n",
    "        line_clean=line_clean+w+\" \"\n",
    "            \n",
    "    if (line_clean!=\"\"):\n",
    "        line_clean=line_clean+\"\\n\"\n",
    "        output_file_clean.write(line_clean)\n",
    "output_file_clean.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_clean = open(path_out+\"0803.2570_clean.txt\", \"r\", encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedata = input_file_clean.read()\n",
    "tokens = filedata.split()\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = fdist.most_common(len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(path_out+'0803.2570_tf.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow([\"word\", \"frecuency\"])\n",
    "    writer.writerows(word_freq)\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 30 words\n",
    "top_words = word_freq[:20]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(top_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x,y = zip(*top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(df[0],df[1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
