{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation with apache spark - pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "# los objetos sc y spark ya pueden estar pre-cargados en otros ambientes como Amazon EMR, Databricks, hortonworks, \n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directorios (path) de entrada y salida:\n",
    "# \n",
    "path_in=\"../datasets/papers_sample/\"\n",
    "#\n",
    "# crear este directorio /tmp/papers_out\n",
    "path_out=\"/tmp/papers_out/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/emontoya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emontoya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus de nltk para 'tokenizer' y 'stopwords'\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             tokens1|longitud1|\n",
      "+--------------------+---------+\n",
      "|[1, , variations,...|    12175|\n",
      "|[smooth, rényi, ...|     4242|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|             tokens2|longitud2|\n",
      "+--------------------+---------+\n",
      "|[1, , variations,...|     8392|\n",
      "|[smooth, rényi, ...|     3122|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|             tokens3|longitud3|\n",
      "+--------------------+---------+\n",
      "|[variations, them...|     6196|\n",
      "|[smooth, rényi, ...|     2243|\n",
      "+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf,col,lower\n",
    "#read the data\n",
    "myrdd = sc.wholeTextFiles(path_in+\"*.txt\")\n",
    "#transform into a data frame\n",
    "mydf = myrdd.toDF(schema=['file','content'])\n",
    "\n",
    "contar = udf(lambda tokens: len(tokens))\n",
    "\n",
    "#tokenize\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"tokens1\")\n",
    "dftokens = tokenizer.transform(mydf)\n",
    "dftokens = dftokens.withColumn('longitud1',contar(col('tokens1'))).select(['tokens1','longitud1'])\n",
    "dftokens.show(2)\n",
    "\n",
    "removersw = StopWordsRemover(inputCol='tokens1', outputCol='tokens2')\n",
    "dftokens = removersw.transform(dftokens)\n",
    "dftokens = dftokens.withColumn('longitud2',contar(col('tokens2'))).select(['tokens2','longitud2'])\n",
    "\n",
    "dftokens.show(2)\n",
    "\n",
    "filter_udp = udf(lambda x: [w for w in x if len(w) > 1])\n",
    "\n",
    "\n",
    "dftokens = dftokens.withColumn('tokens3',filter_udp(col('tokens2')))\n",
    "dftokens = dftokens.withColumn('longitud3',contar(col('tokens3'))).select(['tokens3','longitud3'])\n",
    "dftokens.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo de como nltk tokeniza:\n",
    "texto=\"texto libre weren't que permite--4 crear  las   hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note la estrategia de tokenizar con sentencias simples de python, \n",
    "# ¿ cual le parece mejor?\n",
    "# y note la diferencia entre .split() y .split(' ')\n",
    "texto=\"texto libre weren't que permite--4 crear  las   hiso1iras epor--4 no se preocupe \\n hola mundo cruel\"\n",
    "tokens = texto.split()\n",
    "print(len(tokens))\n",
    "print(tokens)\n",
    "tokens = texto.split(' ')\n",
    "print(len(tokens))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# otra libreria diferentes de nltk para diccionario de stopwords, cual será mejor?\n",
    "# $ git clone --recursive git://github.com/Alir3z4/python-stop-words.git\n",
    "#!pip install stop-words --user\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('english')\n",
    "stop_words = get_stop_words('en')\n",
    "print(len(stop_words))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords en nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(len(stop_words_nltk))\n",
    "print(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permite verificar en nltk si un token pertenece a diccionario de un idioma, en este caso a 'english'\n",
    "from nltk.corpus import words as voc_en\n",
    "x = len(voc_en.words())\n",
    "print('tamaño del diccionario en ingles del nltk: ',x)\n",
    "# verifica si una palabra pertenece al diccionario:\n",
    "w = \"house\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")\n",
    "    \n",
    "w = \"pepito\"\n",
    "if (len(w) >1) and w.isalpha() and (w in voc_en.words()) and (w not in stop_words):\n",
    "    print(w,\" true\")\n",
    "else:\n",
    "    print(w,\" false\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer un archivo de ejemplo en .txt\n",
    "input_file = open(path_in+\"0704.3504.txt\", \"r\", encoding='iso-8859-1')\n",
    "#output_file_filtered = open(path_out+\"0704.3504_filtered.txt\", \"w\")\n",
    "#output_file_drop = open(path_out+\"0704.3504_drop.txt\", \"w\")\n",
    "filedata = input_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 1:\n",
    "# TOKENIZAR con .split(), \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos y pasar todo a minuscula\n",
    "# REMOVER stop words de nltk\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = filedata.split()\n",
    "print(len(tokens))\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "# tokens=[word for word in tokens if word.isalpha()] si en vez de re.sub(r'[^A-Za-z0-9]+','',w) hace esto, que pasa?\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "topwords = fdist.most_common(20)\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opción 2:\n",
    "# TOKENIZAR con nltk, \n",
    "# ELIMINAR tokens de long = 1\n",
    "# ELIMINAR caracteres que no sean alfanumericos\n",
    "# REMOVER stop words\n",
    "# graficar los 20 términos más frecuentes:\n",
    "\n",
    "tokens = nltk.word_tokenize(filedata)\n",
    "tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "print (topwords)\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming con NLTK\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "#tokens = [porter.stem(w) for w in tokens]\n",
    "tokens = [lancaster.stem(w) for w in tokens]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization con NLTK\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = [wordnet_lemmatizer.lemmatize(w, pos=\"v\") for w in tokens ]\n",
    "#tokens = [wordnet_lemmatizer.lemmatize(w) for w in tokens ]\n",
    "\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leer un archivo de ejemplo en .txt\n",
    "input_file = open(path_in+\"0704.3504.txt\", \"r\", encoding='iso-8859-1')\n",
    "output_file_clean = open(path_out+\"0704.3504_clean.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in input_file:\n",
    "    line_clean = \"\"\n",
    "    \n",
    "    tokens = line.split()\n",
    "    tokens = [re.sub(r'[^A-Za-z0-9]+','',w) for w in tokens]\n",
    "    tokens = [w.lower() for w in tokens if len(w)>1]\n",
    "    tokens = [w for w in tokens if w.isalpha()]\n",
    "    tokens = [w for w in tokens if w not in stop_words_nltk]\n",
    "    \n",
    "    for w in tokens:\n",
    "        line_clean=line_clean+w+\" \"\n",
    "            \n",
    "    if (line_clean!=\"\"):\n",
    "        line_clean=line_clean+\"\\n\"\n",
    "        output_file_clean.write(line_clean)\n",
    "output_file_clean.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_clean = open(path_out+\"0704.3504_clean.txt\", \"r\", encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedata = input_file_clean.read()\n",
    "tokens = filedata.split()\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "topwords = fdist.most_common(20)\n",
    "print('numero de palabras finales = ',len(fdist))\n",
    "x,y = zip(*topwords)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = fdist.most_common(len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(path_out+'0704.3504_tf.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow([\"word\", \"frecuency\"])\n",
    "    writer.writerows(word_freq)\n",
    "\n",
    "csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract top 30 words\n",
    "top_words = word_freq[:20]\n",
    "print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(top_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x,y = zip(*top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df = pd.DataFrame(top_words)\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(df[0],df[1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"frecuency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
